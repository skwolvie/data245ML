{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "top2vec 1.0.27 requires gensim>=4.0.0, which is not installed.\n",
      "top2vec 1.0.27 requires wordcloud, which is not installed."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycaret\n",
      "  Downloading pycaret-3.0.2-py3-none-any.whl (483 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in c:\\users\\31405.isbdomain1\\appdata\\roaming\\python\\python38\\site-packages (from pycaret) (1.1.0)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.3.0 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from pycaret) (1.3.1)\n",
      "Collecting numba>=0.55.0\n",
      "  Using cached numba-0.57.0-cp38-cp38-win_amd64.whl (2.6 MB)\n",
      "Requirement already satisfied: ipython>=5.5.0 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from pycaret) (7.22.0)\n",
      "Collecting deprecation>=2.1.0\n",
      "  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting schemdraw==0.15\n",
      "  Downloading schemdraw-0.15-py3-none-any.whl (106 kB)\n",
      "Collecting pyod>=1.0.8\n",
      "  Using cached pyod-1.0.9-py3-none-any.whl\n",
      "Collecting tqdm>=4.62.0\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: jinja2>=1.2 in c:\\users\\31405.isbdomain1\\appdata\\roaming\\python\\python38\\site-packages (from pycaret) (3.1.2)\n",
      "Collecting imbalanced-learn>=0.8.1\n",
      "  Using cached imbalanced_learn-0.10.1-py3-none-any.whl (226 kB)\n",
      "Collecting cloudpickle\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: sktime!=0.17.1,<0.17.2,>=0.16.1 in c:\\users\\31405.isbdomain1\\appdata\\roaming\\python\\python38\\site-packages (from pycaret) (0.17.0)\n",
      "Requirement already satisfied: scikit-plot>=0.3.7 in c:\\users\\31405.isbdomain1\\appdata\\roaming\\python\\python38\\site-packages (from pycaret) (0.3.7)\n",
      "Collecting ipywidgets>=7.6.5\n",
      "  Using cached ipywidgets-8.0.6-py3-none-any.whl (138 kB)\n",
      "Requirement already satisfied: markupsafe>=2.0.1 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from pycaret) (2.0.1)\n",
      "Collecting numpy<1.24,>=1.21\n",
      "  Downloading numpy-1.23.5-cp38-cp38-win_amd64.whl (14.7 MB)\n",
      "Collecting kaleido>=0.2.1\n",
      "  Downloading kaleido-0.2.1-py2.py3-none-win_amd64.whl (65.9 MB)\n",
      "Collecting pmdarima!=1.8.1,<3.0.0,>=1.8.0\n",
      "  Using cached pmdarima-2.0.3-cp38-cp38-win_amd64.whl (572 kB)\n",
      "Collecting lightgbm>=3.0.0\n",
      "  Using cached lightgbm-3.3.5-py3-none-win_amd64.whl (1.0 MB)\n",
      "Collecting statsmodels>=0.12.1\n",
      "  Downloading statsmodels-0.14.0-cp38-cp38-win_amd64.whl (9.4 MB)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\31405.isbdomain1\\appdata\\roaming\\python\\python38\\site-packages (from pycaret) (3.5.1)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp38-cp38-win_amd64.whl (30 kB)\n",
      "Collecting yellowbrick>=1.4\n",
      "  Using cached yellowbrick-1.5-py3-none-any.whl (282 kB)\n",
      "Collecting psutil>=5.9.0\n",
      "  Downloading psutil-5.9.5-cp36-abi3-win_amd64.whl (255 kB)\n",
      "Collecting requests>=2.27.1\n",
      "  Using cached requests-2.30.0-py3-none-any.whl (62 kB)\n",
      "Collecting category-encoders>=2.4.0\n",
      "  Downloading category_encoders-2.6.1-py2.py3-none-any.whl (81 kB)\n",
      "Collecting importlib-metadata>=4.12.0\n",
      "  Downloading importlib_metadata-6.6.0-py3-none-any.whl (22 kB)\n",
      "Collecting nbformat>=4.2.0\n",
      "  Downloading nbformat-5.8.0-py3-none-any.whl (77 kB)\n",
      "Collecting plotly>=5.0.0\n",
      "  Using cached plotly-5.14.1-py2.py3-none-any.whl (15.3 MB)\n",
      "Collecting plotly-resampler>=0.8.3.1\n",
      "  Using cached plotly_resampler-0.8.3.2-cp38-cp38-win_amd64.whl\n",
      "Requirement already satisfied: scipy<2.0.0 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from pycaret) (1.7.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from pycaret) (1.2.0)\n",
      "Collecting tbats>=1.1.3\n",
      "  Using cached tbats-1.1.3-py3-none-any.whl (44 kB)\n",
      "Collecting patsy>=0.5.1\n",
      "  Downloading patsy-0.5.3-py2.py3-none-any.whl (233 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from deprecation>=2.1.0->pycaret) (21.3)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "spacy 2.3.9 requires blis<0.8.0,>=0.4.0, which is not installed.\n",
      "spacy 2.3.9 requires catalogue<1.1.0,>=0.0.7, which is not installed.\n",
      "spacy 2.3.9 requires cymem<2.1.0,>=2.0.2, which is not installed.\n",
      "spacy 2.3.9 requires murmurhash<1.1.0,>=0.28.0, which is not installed.\n",
      "spacy 2.3.9 requires plac<1.2.0,>=0.9.6, which is not installed.\n",
      "spacy 2.3.9 requires preshed<3.1.0,>=3.0.2, which is not installed.\n",
      "spacy 2.3.9 requires srsly<1.1.0,>=1.0.2, which is not installed.\n",
      "spacy 2.3.9 requires thinc<7.5.0,>=7.4.1, which is not installed.\n",
      "spacy 2.3.9 requires wasabi<1.1.0,>=0.4.0, which is not installed.\n",
      "scikit-optimize 0.9.0 requires pyaml>=16.9, which is not installed.\n",
      "pyldavis 3.2.2 requires funcy, which is not installed.\n",
      "pyldavis 3.2.2 requires future, which is not installed.\n",
      "pyldavis 3.2.2 requires numexpr, which is not installed.\n",
      "octis 1.10.3 requires gensim>=4.0.0, which is not installed.\n",
      "octis 1.10.3 requires libsvm, which is not installed.\n",
      "octis 1.10.3 requires tomotopy, which is not installed.\n",
      "mlflow 1.25.1 requires entrypoints, which is not installed.\n",
      "mlflow 1.25.1 requires querystring-parser, which is not installed.\n",
      "mlflow 1.25.1 requires sqlparse>=0.3.1, which is not installed.\n",
      "mlflow 1.25.1 requires waitress; platform_system == \"Windows\", which is not installed.\n",
      "docker 5.0.3 requires websocket-client>=0.32.0, which is not installed.\n",
      "databricks-cli 0.16.6 requires pyjwt>=1.7.0, which is not installed.\n",
      "databricks-cli 0.16.6 requires tabulate>=0.7.7, which is not installed.\n",
      "cufflinks 0.17.3 requires colorlover>=0.2.1, which is not installed.\n",
      "octis 1.10.3 requires scikit-learn==0.24.2, but you have scikit-learn 1.1.0 which is incompatible.\n",
      "tensorflow 2.5.0 requires numpy~=1.19.2, but you have numpy 1.22.4 which is incompatible.\n",
      "tensorflow 2.5.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from imbalanced-learn>=0.8.1->pycaret) (3.1.0)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from ipython>=5.5.0->pycaret) (0.4.4)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from ipython>=5.5.0->pycaret) (3.0.17)\n",
      "Requirement already satisfied: pygments in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from ipython>=5.5.0->pycaret) (2.9.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from ipython>=5.5.0->pycaret) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from ipython>=5.5.0->pycaret) (52.0.0.post20210125)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from ipython>=5.5.0->pycaret) (5.0.5)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from ipython>=5.5.0->pycaret) (0.17.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from ipython>=5.5.0->pycaret) (5.0.9)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from ipython>=5.5.0->pycaret) (0.7.5)\n",
      "Collecting jupyterlab-widgets~=3.0.7\n",
      "  Downloading jupyterlab_widgets-3.0.7-py3-none-any.whl (198 kB)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from ipywidgets>=7.6.5->pycaret) (5.3.4)\n",
      "Collecting widgetsnbextension~=4.0.7\n",
      "  Downloading widgetsnbextension-4.0.7-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: tornado>=4.2 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (6.1)\n",
      "Requirement already satisfied: jupyter-client in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (6.1.12)\n",
      "Requirement already satisfied: parso>=0.7.0 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from jedi>=0.16->ipython>=5.5.0->pycaret) (0.8.2)\n",
      "Requirement already satisfied: wheel in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from lightgbm>=3.0.0->pycaret) (0.36.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from matplotlib>=3.3.0->pycaret) (3.0.9)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp38-cp38-win_amd64.whl (55 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from matplotlib>=3.3.0->pycaret) (2.8.2)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.39.4-py3-none-any.whl (1.0 MB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from matplotlib>=3.3.0->pycaret) (9.3.0)\n",
      "Collecting traitlets>=4.2\n",
      "  Downloading traitlets-5.9.0-py3-none-any.whl (117 kB)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from nbformat>=4.2.0->pycaret) (4.7.1)\n",
      "Collecting jsonschema>=2.6\n",
      "  Downloading jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
      "Collecting fastjsonschema\n",
      "  Downloading fastjsonschema-2.16.3-py3-none-any.whl (23 kB)\n",
      "Collecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0\n",
      "  Downloading pyrsistent-0.19.3-cp38-cp38-win_amd64.whl (62 kB)\n",
      "Collecting pkgutil-resolve-name>=1.3.10\n",
      "  Downloading pkgutil_resolve_name-1.3.10-py3-none-any.whl (4.7 kB)\n",
      "Collecting attrs>=17.4.0\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting importlib-resources>=1.4.0\n",
      "  Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
      "Collecting llvmlite<0.41,>=0.40.0dev0\n",
      "  Using cached llvmlite-0.40.0-cp38-cp38-win_amd64.whl (27.7 MB)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from pandas<2.0.0,>=1.3.0->pycaret) (2021.1)\n",
      "Requirement already satisfied: six in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from patsy>=0.5.1->category-encoders>=2.4.0->pycaret) (1.16.0)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Using cached tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Collecting dash<3.0.0,>=2.2.0\n",
      "  Using cached dash-2.9.3-py3-none-any.whl (10.2 MB)\n",
      "Collecting orjson<4.0.0,>=3.8.0\n",
      "  Downloading orjson-3.8.12-cp38-none-win_amd64.whl (194 kB)\n",
      "Collecting trace-updater>=0.0.8\n",
      "  Downloading trace_updater-0.0.9.1-py3-none-any.whl (185 kB)\n",
      "Collecting jupyter-dash>=0.4.2\n",
      "  Using cached jupyter_dash-0.4.2-py3-none-any.whl (23 kB)\n",
      "Collecting pandas<2.0.0,>=1.3.0\n",
      "  Downloading pandas-1.5.3-cp38-cp38-win_amd64.whl (11.0 MB)\n",
      "Collecting dash-core-components==2.0.0\n",
      "  Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
      "Collecting dash-table==5.0.0\n",
      "  Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
      "Collecting dash-html-components==2.0.0\n",
      "  Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
      "Requirement already satisfied: Flask>=1.0.4 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from dash<3.0.0,>=2.2.0->plotly-resampler>=0.8.3.1->pycaret) (2.0.1)\n",
      "Requirement already satisfied: Werkzeug>=2.0 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from Flask>=1.0.4->dash<3.0.0,>=2.2.0->plotly-resampler>=0.8.3.1->pycaret) (2.0.1)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from Flask>=1.0.4->dash<3.0.0,>=2.2.0->plotly-resampler>=0.8.3.1->pycaret) (2.0.1)\n",
      "Requirement already satisfied: click>=7.1.2 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from Flask>=1.0.4->dash<3.0.0,>=2.2.0->plotly-resampler>=0.8.3.1->pycaret) (8.0.1)\n",
      "Collecting nest-asyncio\n",
      "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
      "Collecting retrying\n",
      "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
      "Collecting ansi2html\n",
      "  Downloading ansi2html-1.8.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from pmdarima!=1.8.1,<3.0.0,>=1.8.0->pycaret) (1.26.6)\n",
      "Collecting Cython!=0.29.18,!=0.29.31,>=0.29\n",
      "  Downloading Cython-0.29.34-py2.py3-none-any.whl (988 kB)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.5.0->pycaret) (0.2.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from requests>=2.27.1->pycaret) (3.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from requests>=2.27.1->pycaret) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from requests>=2.27.1->pycaret) (2021.5.30)\n",
      "Collecting numpy<1.24,>=1.21\n",
      "  Downloading numpy-1.22.4-cp38-cp38-win_amd64.whl (14.8 MB)\n",
      "Collecting deprecated>=1.2.13\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from deprecated>=1.2.13->sktime!=0.17.1,<0.17.2,>=0.16.1->pycaret) (1.12.1)\n",
      "Requirement already satisfied: pyzmq>=13 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (20.0.0)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\31405.isbdomain1\\anaconda3\\envs\\eth\\lib\\site-packages (from jupyter-core->nbformat>=4.2.0->pycaret) (227)\n",
      "Installing collected packages: traitlets, tenacity, numpy, zipp, plotly, patsy, pandas, dash-table, dash-html-components, dash-core-components, statsmodels, retrying, requests, pyrsistent, pkgutil-resolve-name, nest-asyncio, llvmlite, kiwisolver, importlib-resources, importlib-metadata, fonttools, dash, Cython, cycler, attrs, ansi2html, widgetsnbextension, trace-updater, pmdarima, orjson, numba, jupyterlab-widgets, jupyter-dash, jsonschema, fastjsonschema, deprecated, yellowbrick, xxhash, tqdm, tbats, schemdraw, pyod, psutil, plotly-resampler, nbformat, lightgbm, kaleido, ipywidgets, imbalanced-learn, deprecation, cloudpickle, category-encoders, pycaret\n",
      "  Attempting uninstall: traitlets\n",
      "    Found existing installation: traitlets 5.0.5\n",
      "    Uninstalling traitlets-5.0.5:\n",
      "      Successfully uninstalled traitlets-5.0.5\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.1\n",
      "    Uninstalling pandas-1.3.1:\n",
      "      Successfully uninstalled pandas-1.3.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.26.0\n",
      "    Uninstalling requests-2.26.0:\n",
      "      Successfully uninstalled requests-2.26.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.61.2\n",
      "    Uninstalling tqdm-4.61.2:\n",
      "      Successfully uninstalled tqdm-4.61.2\n",
      "Successfully installed Cython-0.29.34 ansi2html-1.8.0 attrs-23.1.0 category-encoders-2.6.1 cloudpickle-2.2.1 cycler-0.11.0 dash-2.9.3 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 deprecated-1.2.13 deprecation-2.1.0 fastjsonschema-2.16.3 fonttools-4.39.4 imbalanced-learn-0.10.1 importlib-metadata-6.6.0 importlib-resources-5.12.0 ipywidgets-8.0.6 jsonschema-4.17.3 jupyter-dash-0.4.2 jupyterlab-widgets-3.0.7 kaleido-0.2.1 kiwisolver-1.4.4 lightgbm-3.3.5 llvmlite-0.40.0 nbformat-5.8.0 nest-asyncio-1.5.6 numba-0.57.0 numpy-1.22.4 orjson-3.8.12 pandas-1.5.3 patsy-0.5.3 pkgutil-resolve-name-1.3.10 plotly-5.14.1 plotly-resampler-0.8.3.2 pmdarima-2.0.3 psutil-5.9.5 pycaret-3.0.2 pyod-1.0.9 pyrsistent-0.19.3 requests-2.30.0 retrying-1.3.4 schemdraw-0.15 statsmodels-0.14.0 tbats-1.1.3 tenacity-8.2.2 tqdm-4.65.0 trace-updater-0.0.9.1 traitlets-5.9.0 widgetsnbextension-4.0.7 xxhash-3.2.0 yellowbrick-1.5 zipp-3.15.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 558723 entries, 0 to 558722\n",
      "Data columns (total 41 columns):\n",
      " #   Column                           Non-Null Count   Dtype  \n",
      "---  ------                           --------------   -----  \n",
      " 0   complaint_id                     558723 non-null  int64  \n",
      " 1   submitted_via                    558723 non-null  object \n",
      " 2   date_received                    558723 non-null  object \n",
      " 3   date_sent_to_company             558723 non-null  object \n",
      " 4   timely_response                  558723 non-null  object \n",
      " 5   lag_submit_to_send               558723 non-null  int64  \n",
      " 6   year                             558723 non-null  int64  \n",
      " 7   month                            558723 non-null  int64  \n",
      " 8   week                             558723 non-null  int64  \n",
      " 9   day                              558723 non-null  int64  \n",
      " 10  product                          558723 non-null  object \n",
      " 11  sub_product                      374308 non-null  object \n",
      " 12  issue                            558723 non-null  object \n",
      " 13  sub_issue                        229740 non-null  object \n",
      " 14  company                          558723 non-null  object \n",
      " 15  state                            558723 non-null  object \n",
      " 16  consumer_consent_provided        214270 non-null  object \n",
      " 17  company_response_to_consumer     558723 non-null  object \n",
      " 18  consumer_disputed                558723 non-null  object \n",
      " 19  submitted_via_le                 558723 non-null  int64  \n",
      " 20  timely_response_le               558723 non-null  int64  \n",
      " 21  lag_submit_to_send_mms           558723 non-null  float64\n",
      " 22  product_le                       558723 non-null  int64  \n",
      " 23  sub_product_le                   374308 non-null  float64\n",
      " 24  issue_le                         558723 non-null  int64  \n",
      " 25  sub_issue_le                     229740 non-null  float64\n",
      " 26  company_ce                       558723 non-null  float64\n",
      " 27  state_le                         558723 non-null  int64  \n",
      " 28  company_response_to_consumer_le  558723 non-null  int64  \n",
      " 29  consumer_consent_provided_le     214270 non-null  float64\n",
      " 30  consumer_disputed_le             558723 non-null  int64  \n",
      " 31  PCA_1                            558723 non-null  float64\n",
      " 32  PCA_2                            558723 non-null  float64\n",
      " 33  PCA_3                            558723 non-null  float64\n",
      " 34  PCA_4                            558723 non-null  float64\n",
      " 35  PCA_5                            558723 non-null  float64\n",
      " 36  PCA_6                            558723 non-null  float64\n",
      " 37  PCA_7                            558723 non-null  float64\n",
      " 38  PCA_8                            558723 non-null  float64\n",
      " 39  PCA_9                            558723 non-null  float64\n",
      " 40  PCA_10                           558723 non-null  float64\n",
      "dtypes: float64(15), int64(13), object(13)\n",
      "memory usage: 174.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv('data/complaints_for_modeling.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_response_to_consumer</th>\n",
       "      <th>company_response_to_consumer_le</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Closed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Closed with monetary relief</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Closed with non-monetary relief</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      company_response_to_consumer  company_response_to_consumer_le\n",
       "0                           Closed                                0\n",
       "1          Closed with explanation                                1\n",
       "4      Closed with monetary relief                                2\n",
       "6  Closed with non-monetary relief                                3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['company_response_to_consumer', 'company_response_to_consumer_le']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['submitted_via_le', 'timely_response_le',\n",
    "       'year', 'month', 'day',\n",
    "       'product_le', 'sub_product_le', 'issue_le', 'sub_issue_le', 'company_ce', 'state_le', \n",
    "       'consumer_consent_provided_le']]\n",
    "y = df['company_response_to_consumer_le']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pycaret\n",
    "pycaret.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function setup in module pycaret.classification.functional:\n",
      "\n",
      "setup(data: Union[dict, list, tuple, numpy.ndarray, scipy.sparse.base.spmatrix, pandas.core.frame.DataFrame, NoneType] = None, data_func: Union[Callable[[], Union[dict, list, tuple, numpy.ndarray, scipy.sparse.base.spmatrix, pandas.core.frame.DataFrame]], NoneType] = None, target: Union[int, str, list, tuple, numpy.ndarray, pandas.core.series.Series] = -1, index: Union[bool, int, str, list, tuple, numpy.ndarray, pandas.core.series.Series] = True, train_size: float = 0.7, test_data: Union[dict, list, tuple, numpy.ndarray, scipy.sparse.base.spmatrix, pandas.core.frame.DataFrame, NoneType] = None, ordinal_features: Union[Dict[str, list], NoneType] = None, numeric_features: Union[List[str], NoneType] = None, categorical_features: Union[List[str], NoneType] = None, date_features: Union[List[str], NoneType] = None, text_features: Union[List[str], NoneType] = None, ignore_features: Union[List[str], NoneType] = None, keep_features: Union[List[str], NoneType] = None, preprocess: bool = True, create_date_columns: List[str] = ['day', 'month', 'year'], imputation_type: Union[str, NoneType] = 'simple', numeric_imputation: Union[int, float, str] = 'mean', categorical_imputation: str = 'mode', iterative_imputation_iters: int = 5, numeric_iterative_imputer: Union[str, Any] = 'lightgbm', categorical_iterative_imputer: Union[str, Any] = 'lightgbm', text_features_method: str = 'tf-idf', max_encoding_ohe: int = 25, encoding_method: Union[Any, NoneType] = None, rare_to_value: Union[float, NoneType] = None, rare_value: str = 'rare', polynomial_features: bool = False, polynomial_degree: int = 2, low_variance_threshold: Union[float, NoneType] = None, group_features: Union[list, NoneType] = None, group_names: Union[str, list, NoneType] = None, drop_groups: bool = False, remove_multicollinearity: bool = False, multicollinearity_threshold: float = 0.9, bin_numeric_features: Union[List[str], NoneType] = None, remove_outliers: bool = False, outliers_method: str = 'iforest', outliers_threshold: float = 0.05, fix_imbalance: bool = False, fix_imbalance_method: Union[str, Any] = 'SMOTE', transformation: bool = False, transformation_method: str = 'yeo-johnson', normalize: bool = False, normalize_method: str = 'zscore', pca: bool = False, pca_method: str = 'linear', pca_components: Union[int, float, str, NoneType] = None, feature_selection: bool = False, feature_selection_method: str = 'classic', feature_selection_estimator: Union[str, Any] = 'lightgbm', n_features_to_select: Union[int, float] = 0.2, custom_pipeline: Union[Any, NoneType] = None, custom_pipeline_position: int = -1, data_split_shuffle: bool = True, data_split_stratify: Union[bool, List[str]] = True, fold_strategy: Union[str, Any] = 'stratifiedkfold', fold: int = 10, fold_shuffle: bool = False, fold_groups: Union[str, pandas.core.frame.DataFrame, NoneType] = None, n_jobs: Union[int, NoneType] = -1, use_gpu: bool = False, html: bool = True, session_id: Union[int, NoneType] = None, system_log: Union[bool, str, logging.Logger] = True, log_experiment: Union[bool, str, pycaret.loggers.base_logger.BaseLogger, List[Union[str, pycaret.loggers.base_logger.BaseLogger]]] = False, experiment_name: Union[str, NoneType] = None, experiment_custom_tags: Union[Dict[str, Any], NoneType] = None, log_plots: Union[bool, list] = False, log_profile: bool = False, log_data: bool = False, verbose: bool = True, memory: Union[bool, str, joblib.memory.Memory] = True, profile: bool = False, profile_kwargs: Union[Dict[str, Any], NoneType] = None)\n",
      "    This function initializes the training environment and creates the transformation\n",
      "    pipeline. Setup function must be called before executing any other function. It takes\n",
      "    two mandatory parameters: ``data`` and ``target``. All the other parameters are\n",
      "    optional.\n",
      "    \n",
      "    Example\n",
      "    -------\n",
      "    >>> from pycaret.datasets import get_data\n",
      "    >>> juice = get_data('juice')\n",
      "    >>> from pycaret.classification import *\n",
      "    >>> exp_name = setup(data = juice,  target = 'Purchase')\n",
      "    \n",
      "    \n",
      "    data: dataframe-like = None\n",
      "        Data set with shape (n_samples, n_features), where n_samples is the\n",
      "        number of samples and n_features is the number of features. If data\n",
      "        is not a pandas dataframe, it's converted to one using default column\n",
      "        names.\n",
      "    \n",
      "    \n",
      "    data_func: Callable[[], DATAFRAME_LIKE] = None\n",
      "        The function that generate ``data`` (the dataframe-like input). This\n",
      "        is useful when the dataset is large, and you need parallel operations\n",
      "        such as ``compare_models``. It can avoid broadcasting large dataset\n",
      "        from driver to workers. Notice one and only one of ``data`` and\n",
      "        ``data_func`` must be set.\n",
      "    \n",
      "    \n",
      "    target: int, str or sequence, default = -1\n",
      "        If int or str, respectivcely index or name of the target column in data.\n",
      "        The default value selects the last column in the dataset. If sequence,\n",
      "        it should have shape (n_samples,). The target can be either binary or\n",
      "        multiclass.\n",
      "    \n",
      "    \n",
      "    index: bool, int, str or sequence, default = True\n",
      "        Handle indices in the `data` dataframe.\n",
      "            - If False: Reset to RangeIndex.\n",
      "            - If True: Keep the provided index.\n",
      "            - If int: Position of the column to use as index.\n",
      "            - If str: Name of the column to use as index.\n",
      "            - If sequence: Array with shape=(n_samples,) to use as index.\n",
      "    \n",
      "    \n",
      "    train_size: float, default = 0.7\n",
      "        Proportion of the dataset to be used for training and validation. Should be\n",
      "        between 0.0 and 1.0.\n",
      "    \n",
      "    \n",
      "    test_data: dataframe-like or None, default = None\n",
      "        If not None, test_data is used as a hold-out set and `train_size` parameter\n",
      "        is ignored. The columns of data and test_data must match.\n",
      "    \n",
      "    \n",
      "    ordinal_features: dict, default = None\n",
      "        Categorical features to be encoded ordinally. For example, a categorical\n",
      "        feature with 'low', 'medium', 'high' values where low < medium < high can\n",
      "        be passed as ordinal_features = {'column_name' : ['low', 'medium', 'high']}.\n",
      "    \n",
      "    \n",
      "    numeric_features: list of str, default = None\n",
      "        If the inferred data types are not correct, the numeric_features param can\n",
      "        be used to define the data types. It takes a list of strings with column\n",
      "        names that are numeric.\n",
      "    \n",
      "    \n",
      "    categorical_features: list of str, default = None\n",
      "        If the inferred data types are not correct, the categorical_features param\n",
      "        can be used to define the data types. It takes a list of strings with column\n",
      "        names that are categorical.\n",
      "    \n",
      "    \n",
      "    date_features: list of str, default = None\n",
      "        If the inferred data types are not correct, the date_features param can be\n",
      "        used to overwrite the data types. It takes a list of strings with column\n",
      "        names that are DateTime.\n",
      "    \n",
      "    \n",
      "    text_features: list of str, default = None\n",
      "        Column names that contain a text corpus. If None, no text features are\n",
      "        selected.\n",
      "    \n",
      "    \n",
      "    ignore_features: list of str, default = None\n",
      "        ignore_features param can be used to ignore features during preprocessing\n",
      "        and model training. It takes a list of strings with column names that are\n",
      "        to be ignored.\n",
      "    \n",
      "    \n",
      "    keep_features: list of str, default = None\n",
      "        keep_features param can be used to always keep specific features during\n",
      "        preprocessing, i.e. these features are never dropped by any kind of\n",
      "        feature selection. It takes a list of strings with column names that are\n",
      "        to be kept.\n",
      "    \n",
      "    \n",
      "    preprocess: bool, default = True\n",
      "        When set to False, no transformations are applied except for train_test_split\n",
      "        and custom transformations passed in ``custom_pipeline`` param. Data must be\n",
      "        ready for modeling (no missing values, no dates, categorical data encoding),\n",
      "        when preprocess is set to False.\n",
      "    \n",
      "    \n",
      "    create_date_columns: list of str, default = [\"day\", \"month\", \"year\"]\n",
      "        Columns to create from the date features. Note that created features\n",
      "        with zero variance (e.g. the feature hour in a column that only contains\n",
      "        dates) are ignored. Allowed values are datetime attributes from\n",
      "        `pandas.Series.dt`. The datetime format of the feature is inferred\n",
      "        automatically from the first non NaN value.\n",
      "    \n",
      "    \n",
      "    imputation_type: str or None, default = 'simple'\n",
      "        The type of imputation to use. Can be either 'simple' or 'iterative'.\n",
      "        If None, no imputation of missing values is performed.\n",
      "    \n",
      "    \n",
      "    numeric_imputation: int, float or str, default = 'mean'\n",
      "        Imputing strategy for numerical columns. Ignored when ``imputation_type=\n",
      "        iterative``. Choose from:\n",
      "            - \"drop\": Drop rows containing missing values.\n",
      "            - \"mean\": Impute with mean of column.\n",
      "            - \"median\": Impute with median of column.\n",
      "            - \"mode\": Impute with most frequent value.\n",
      "            - \"knn\": Impute using a K-Nearest Neighbors approach.\n",
      "            - int or float: Impute with provided numerical value.\n",
      "    \n",
      "    \n",
      "    categorical_imputation: str, default = 'mode'\n",
      "        Imputing strategy for categorical columns. Ignored when ``imputation_type=\n",
      "        iterative``. Choose from:\n",
      "            - \"drop\": Drop rows containing missing values.\n",
      "            - \"mode\": Impute with most frequent value.\n",
      "            - str: Impute with provided string.\n",
      "    \n",
      "    \n",
      "    iterative_imputation_iters: int, default = 5\n",
      "        Number of iterations. Ignored when ``imputation_type=simple``.\n",
      "    \n",
      "    \n",
      "    numeric_iterative_imputer: str or sklearn estimator, default = 'lightgbm'\n",
      "        Regressor for iterative imputation of missing values in numeric features.\n",
      "        If None, it uses LGBClassifier. Ignored when ``imputation_type=simple``.\n",
      "    \n",
      "    \n",
      "    categorical_iterative_imputer: str or sklearn estimator, default = 'lightgbm'\n",
      "        Regressor for iterative imputation of missing values in categorical features.\n",
      "        If None, it uses LGBClassifier. Ignored when ``imputation_type=simple``.\n",
      "    \n",
      "    \n",
      "    text_features_method: str, default = \"tf-idf\"\n",
      "        Method with which to embed the text features in the dataset. Choose\n",
      "        between \"bow\" (Bag of Words - CountVectorizer) or \"tf-idf\" (TfidfVectorizer).\n",
      "        Be aware that the sparse matrix output of the transformer is converted\n",
      "        internally to its full array. This can cause memory issues for large\n",
      "        text embeddings.\n",
      "    \n",
      "    \n",
      "    max_encoding_ohe: int, default = 25\n",
      "        Categorical columns with `max_encoding_ohe` or less unique values are\n",
      "        encoded using OneHotEncoding. If more, the `encoding_method` estimator\n",
      "        is used. Note that columns with exactly two classes are always encoded\n",
      "        ordinally. Set to below 0 to always use OneHotEncoding.\n",
      "    \n",
      "    \n",
      "    encoding_method: category-encoders estimator, default = None\n",
      "        A `category-encoders` estimator to encode the categorical columns\n",
      "        with more than `max_encoding_ohe` unique values. If None,\n",
      "        `category_encoders.target_encoder.TargetEncoder` is used.\n",
      "    \n",
      "    \n",
      "    rare_to_value: float or None, default=None\n",
      "        Minimum fraction of category occurrences in a categorical column.\n",
      "        If a category is less frequent than `rare_to_value * len(X)`, it is\n",
      "        replaced with the string in `rare_value`. Use this parameter to group\n",
      "        rare categories before encoding the column. If None, ignores this step.\n",
      "    \n",
      "    \n",
      "    rare_value: str, default=\"rare\"\n",
      "        Value with which to replace rare categories. Ignored when\n",
      "        ``rare_to_value`` is None.\n",
      "    \n",
      "    \n",
      "    polynomial_features: bool, default = False\n",
      "        When set to True, new features are derived using existing numeric features.\n",
      "    \n",
      "    \n",
      "    polynomial_degree: int, default = 2\n",
      "        Degree of polynomial features. For example, if an input sample is two dimensional\n",
      "        and of the form [a, b], the polynomial features with degree = 2 are:\n",
      "        [1, a, b, a^2, ab, b^2]. Ignored when ``polynomial_features`` is not True.\n",
      "    \n",
      "    \n",
      "    low_variance_threshold: float or None, default = None\n",
      "        Remove features with a training-set variance lower than the provided\n",
      "        threshold. If 0, keep all features with non-zero variance, i.e. remove\n",
      "        the features that have the same value in all samples. If None, skip\n",
      "        this transformation step.\n",
      "    \n",
      "    \n",
      "    group_features: list, list of lists or None, default = None\n",
      "        When the dataset contains features with related characteristics,\n",
      "        add new fetaures with the following statistical properties of that\n",
      "        group: min, max, mean, std, median and mode. The parameter takes a\n",
      "        list of feature names or a list of lists of feature names to specify\n",
      "        multiple groups.\n",
      "    \n",
      "    group_names: str, list, or None, default = None\n",
      "        Group names to be used when naming the new features. The length\n",
      "        should match with the number of groups specified in ``group_features``.\n",
      "        If None, new features are named using the default form, e.g. group_1,\n",
      "        group_2, etc... Ignored when ``group_features`` is None.\n",
      "    \n",
      "    \n",
      "    drop_groups: bool, default=False\n",
      "        Whether to drop the original features in the group. Ignored when\n",
      "        ``group_features`` is None.\n",
      "    \n",
      "    \n",
      "    remove_multicollinearity: bool, default = False\n",
      "        When set to True, features with the inter-correlations higher than\n",
      "        the defined threshold are removed. For each group, it removes all\n",
      "        except the feature with the highest correlation to `y`.\n",
      "    \n",
      "    \n",
      "    multicollinearity_threshold: float, default = 0.9\n",
      "        Minimum absolute Pearson correlation to identify correlated\n",
      "        features. The default value removes equal columns. Ignored when\n",
      "        ``remove_multicollinearity`` is not True.\n",
      "    \n",
      "    \n",
      "    bin_numeric_features: list of str, default = None\n",
      "        To convert numeric features into categorical, bin_numeric_features parameter can\n",
      "        be used. It takes a list of strings with column names to be discretized. It does\n",
      "        so by using 'sturges' rule to determine the number of clusters and then apply\n",
      "        KMeans algorithm. Original values of the feature are then replaced by the\n",
      "        cluster label.\n",
      "    \n",
      "    \n",
      "    remove_outliers: bool, default = False\n",
      "        When set to True, outliers from the training data are removed using an\n",
      "        Isolation Forest.\n",
      "    \n",
      "    \n",
      "    outliers_method: str, default = \"iforest\"\n",
      "        Method with which to remove outliers. Ignored when `remove_outliers=False`.\n",
      "        Possible values are:\n",
      "            - 'iforest': Uses sklearn's IsolationForest.\n",
      "            - 'ee': Uses sklearn's EllipticEnvelope.\n",
      "            - 'lof': Uses sklearn's LocalOutlierFactor.\n",
      "    \n",
      "    \n",
      "    outliers_threshold: float, default = 0.05\n",
      "        The percentage of outliers to be removed from the dataset. Ignored\n",
      "        when ``remove_outliers=False``.\n",
      "    \n",
      "    \n",
      "    fix_imbalance: bool, default = False\n",
      "        When training dataset has unequal distribution of target class it can be balanced\n",
      "        using this parameter. When set to True, SMOTE (Synthetic Minority Over-sampling\n",
      "        Technique) is applied by default to create synthetic datapoints for minority class.\n",
      "    \n",
      "    \n",
      "    fix_imbalance_method: str or imblearn estimator, default = \"SMOTE\"\n",
      "        Estimator with which to perform class balancing. Choose from the name\n",
      "        of an `imblearn` estimator, or a custom instance of such. Ignored when\n",
      "        `fix_imbalance=False`.\n",
      "    \n",
      "    \n",
      "    transformation: bool, default = False\n",
      "        When set to True, it applies the power transform to make data more Gaussian-like.\n",
      "        Type of transformation is defined by the ``transformation_method`` parameter.\n",
      "    \n",
      "    \n",
      "    transformation_method: str, default = 'yeo-johnson'\n",
      "        Defines the method for transformation. By default, the transformation method is\n",
      "        set to 'yeo-johnson'. The other available option for transformation is 'quantile'.\n",
      "        Ignored when ``transformation`` is not True.\n",
      "    \n",
      "    \n",
      "    normalize: bool, default = False\n",
      "        When set to True, it transforms the features by scaling them to a given\n",
      "        range. Type of scaling is defined by the ``normalize_method`` parameter.\n",
      "    \n",
      "    \n",
      "    normalize_method: str, default = 'zscore'\n",
      "        Defines the method for scaling. By default, normalize method is set to 'zscore'\n",
      "        The standard zscore is calculated as z = (x - u) / s. Ignored when ``normalize``\n",
      "        is not True. The other options are:\n",
      "    \n",
      "        - minmax: scales and translates each feature individually such that it is in\n",
      "          the range of 0 - 1.\n",
      "        - maxabs: scales and translates each feature individually such that the\n",
      "          maximal absolute value of each feature will be 1.0. It does not\n",
      "          shift/center the data, and thus does not destroy any sparsity.\n",
      "        - robust: scales and translates each feature according to the Interquartile\n",
      "          range. When the dataset contains outliers, robust scaler often gives\n",
      "          better results.\n",
      "    \n",
      "    \n",
      "    pca: bool, default = False\n",
      "        When set to True, dimensionality reduction is applied to project the data into\n",
      "        a lower dimensional space using the method defined in ``pca_method`` parameter.\n",
      "    \n",
      "    \n",
      "    pca_method: str, default = 'linear'\n",
      "        Method with which to apply PCA. Possible values are:\n",
      "            - 'linear': Uses Singular Value  Decomposition.\n",
      "            - 'kernel': Dimensionality reduction through the use of RBF kernel.\n",
      "            - 'incremental': Similar to 'linear', but more efficient for large datasets.\n",
      "    \n",
      "    \n",
      "    pca_components: int, float, str or None, default = None\n",
      "        Number of components to keep. This parameter is ignored when `pca=False`.\n",
      "            - If None: All components are kept.\n",
      "            - If int: Absolute number of components.\n",
      "            - If float: Such an amount that the variance that needs to be explained\n",
      "                        is greater than the percentage specified by `n_components`.\n",
      "                        Value should lie between 0 and 1 (ony for pca_method='linear').\n",
      "            - If \"mle\": Minkaâ€™s MLE is used to guess the dimension (ony for pca_method='linear').\n",
      "    \n",
      "    \n",
      "    feature_selection: bool, default = False\n",
      "        When set to True, a subset of features is selected based on a feature\n",
      "        importance score determined by ``feature_selection_estimator``.\n",
      "    \n",
      "    \n",
      "    feature_selection_method: str, default = 'classic'\n",
      "        Algorithm for feature selection. Choose from:\n",
      "            - 'univariate': Uses sklearn's SelectKBest.\n",
      "            - 'classic': Uses sklearn's SelectFromModel.\n",
      "            - 'sequential': Uses sklearn's SequentialFeatureSelector.\n",
      "    \n",
      "    \n",
      "    feature_selection_estimator: str or sklearn estimator, default = 'lightgbm'\n",
      "        Classifier used to determine the feature importances. The\n",
      "        estimator should have a `feature_importances_` or `coef_`\n",
      "        attribute after fitting. If None, it uses LGBClassifier. This\n",
      "        parameter is ignored when `feature_selection_method=univariate`.\n",
      "    \n",
      "    \n",
      "    n_features_to_select: int or float, default = 0.2\n",
      "        The maximum number of features to select with feature_selection. If <1,\n",
      "        it's the fraction of starting features. Note that this parameter doesn't\n",
      "        take features in ``ignore_features`` or ``keep_features`` into account\n",
      "        when counting.\n",
      "    \n",
      "    \n",
      "    custom_pipeline: list of (str, transformer), dict or Pipeline, default = None\n",
      "        Addidiotnal custom transformers. If passed, they are applied to the\n",
      "        pipeline last, after all the build-in transformers.\n",
      "    \n",
      "    \n",
      "    custom_pipeline_position: int, default = -1\n",
      "        Position of the custom pipeline in the overal preprocessing pipeline.\n",
      "        The default value adds the custom pipeline last.\n",
      "    \n",
      "    \n",
      "    data_split_shuffle: bool, default = True\n",
      "        When set to False, prevents shuffling of rows during 'train_test_split'.\n",
      "    \n",
      "    \n",
      "    data_split_stratify: bool or list, default = True\n",
      "        Controls stratification during 'train_test_split'. When set to True, will\n",
      "        stratify by target column. To stratify on any other columns, pass a list of\n",
      "        column names. Ignored when ``data_split_shuffle`` is False.\n",
      "    \n",
      "    \n",
      "    fold_strategy: str or sklearn CV generator object, default = 'stratifiedkfold'\n",
      "        Choice of cross validation strategy. Possible values are:\n",
      "    \n",
      "        * 'kfold'\n",
      "        * 'stratifiedkfold'\n",
      "        * 'groupkfold'\n",
      "        * 'timeseries'\n",
      "        * a custom CV generator object compatible with scikit-learn.\n",
      "    \n",
      "        For ``groupkfold``, column name must be passed in ``fold_groups`` parameter.\n",
      "        Example: ``setup(fold_strategy=\"groupkfold\", fold_groups=\"COLUMN_NAME\")``\n",
      "    \n",
      "    fold: int, default = 10\n",
      "        Number of folds to be used in cross validation. Must be at least 2. This is\n",
      "        a global setting that can be over-written at function level by using ``fold``\n",
      "        parameter. Ignored when ``fold_strategy`` is a custom object.\n",
      "    \n",
      "    \n",
      "    fold_shuffle: bool, default = False\n",
      "        Controls the shuffle parameter of CV. Only applicable when ``fold_strategy``\n",
      "        is 'kfold' or 'stratifiedkfold'. Ignored when ``fold_strategy`` is a custom\n",
      "        object.\n",
      "    \n",
      "    \n",
      "    fold_groups: str or array-like, with shape (n_samples,), default = None\n",
      "        Optional group labels when 'GroupKFold' is used for the cross validation.\n",
      "        It takes an array with shape (n_samples, ) where n_samples is the number\n",
      "        of rows in the training dataset. When string is passed, it is interpreted\n",
      "        as the column name in the dataset containing group labels.\n",
      "    \n",
      "    \n",
      "    n_jobs: int, default = -1\n",
      "        The number of jobs to run in parallel (for functions that supports parallel\n",
      "        processing) -1 means using all processors. To run all functions on single\n",
      "        processor set n_jobs to None.\n",
      "    \n",
      "    \n",
      "    use_gpu: bool or str, default = False\n",
      "        When set to True, it will use GPU for training with algorithms that support it,\n",
      "        and fall back to CPU if they are unavailable. When set to 'force', it will only\n",
      "        use GPU-enabled algorithms and raise exceptions when they are unavailable. When\n",
      "        False, all algorithms are trained using CPU only.\n",
      "    \n",
      "        GPU enabled algorithms:\n",
      "    \n",
      "        - Extreme Gradient Boosting, requires no further installation\n",
      "    \n",
      "        - CatBoost Classifier, requires no further installation\n",
      "          (GPU is only enabled when data > 50,000 rows)\n",
      "    \n",
      "        - Light Gradient Boosting Machine, requires GPU installation\n",
      "          https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html\n",
      "    \n",
      "        - Logistic Regression, Ridge Classifier, Random Forest, K Neighbors Classifier,\n",
      "          Support Vector Machine, requires cuML >= 0.15\n",
      "          https://github.com/rapidsai/cuml\n",
      "    \n",
      "    \n",
      "    html: bool, default = True\n",
      "        When set to False, prevents runtime display of monitor. This must be set to False\n",
      "        when the environment does not support IPython. For example, command line terminal,\n",
      "        Databricks Notebook, Spyder and other similar IDEs.\n",
      "    \n",
      "    \n",
      "    session_id: int, default = None\n",
      "        Controls the randomness of experiment. It is equivalent to 'random_state' in\n",
      "        scikit-learn. When None, a pseudo random number is generated. This can be used\n",
      "        for later reproducibility of the entire experiment.\n",
      "    \n",
      "    \n",
      "    log_experiment: bool or str or BaseLogger or list of str or BaseLogger, default = False\n",
      "        A (list of) PyCaret ``BaseLogger`` or str (one of 'mlflow', 'wandb', 'comet_ml')\n",
      "        corresponding to a logger to determine which experiment loggers to use.\n",
      "        Setting to True will use just MLFlow.\n",
      "    \n",
      "    \n",
      "    system_log: bool or str or logging.Logger, default = True\n",
      "        Whether to save the system logging file (as logs.log). If the input\n",
      "        is a string, use that as the path to the logging file. If the input\n",
      "        already is a logger object, use that one instead.\n",
      "    \n",
      "    \n",
      "    experiment_name: str, default = None\n",
      "        Name of the experiment for logging. Ignored when ``log_experiment`` is False.\n",
      "    \n",
      "    \n",
      "    experiment_custom_tags: dict, default = None\n",
      "        Dictionary of tag_name: String -> value: (String, but will be string-ified\n",
      "        if not) passed to the mlflow.set_tags to add new custom tags for the experiment.\n",
      "    \n",
      "    \n",
      "    log_plots: bool or list, default = False\n",
      "        When set to True, certain plots are logged automatically in the ``MLFlow`` server.\n",
      "        To change the type of plots to be logged, pass a list containing plot IDs. Refer\n",
      "        to documentation of ``plot_model``. Ignored when ``log_experiment`` is False.\n",
      "    \n",
      "    \n",
      "    log_profile: bool, default = False\n",
      "        When set to True, data profile is logged on the ``MLflow`` server as a html file.\n",
      "        Ignored when ``log_experiment`` is False.\n",
      "    \n",
      "    \n",
      "    log_data: bool, default = False\n",
      "        When set to True, dataset is logged on the ``MLflow`` server as a csv file.\n",
      "        Ignored when ``log_experiment`` is False.\n",
      "    \n",
      "    \n",
      "    verbose: bool, default = True\n",
      "        When set to False, Information grid is not printed.\n",
      "    \n",
      "    \n",
      "    memory: str, bool or Memory, default=True\n",
      "        Used to cache the fitted transformers of the pipeline.\n",
      "            If False: No caching is performed.\n",
      "            If True: A default temp directory is used.\n",
      "            If str: Path to the caching directory.\n",
      "    \n",
      "    \n",
      "    profile: bool, default = False\n",
      "        When set to True, an interactive EDA report is displayed.\n",
      "    \n",
      "    \n",
      "    profile_kwargs: dict, default = {} (empty dict)\n",
      "        Dictionary of arguments passed to the ProfileReport method used\n",
      "        to create the EDA report. Ignored if ``profile`` is False.\n",
      "    \n",
      "    \n",
      "    Returns:\n",
      "        ClassificationExperiment object.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import pycaret classification and init setup\n",
    "from pycaret.classification import *\n",
    "help(setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "submitted_via_le                        0\n",
       "timely_response_le                      0\n",
       "year                                    0\n",
       "month                                   0\n",
       "day                                     0\n",
       "product_le                              0\n",
       "sub_product_le                     184415\n",
       "issue_le                                0\n",
       "sub_issue_le                       328983\n",
       "company_ce                              0\n",
       "state_le                                0\n",
       "consumer_consent_provided_le       344453\n",
       "company_response_to_consumer_le         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm=df[['submitted_via_le', 'timely_response_le',\n",
    "       'year', 'month', 'day',\n",
    "       'product_le', 'sub_product_le', 'issue_le', 'sub_issue_le', 'company_ce', 'state_le', \n",
    "       'consumer_consent_provided_le', 'company_response_to_consumer_le']]\n",
    "\n",
    "dfm.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = setup(dfm,\n",
    "       target = 'company_response_to_consumer_le', \n",
    "       categorical_features=['submitted_via_le', 'timely_response_le', 'product_le', 'sub_product_le', 'issue_le', 'sub_issue_le', 'state_le', 'consumer_consent_provided_le'],\n",
    "       session_id = 125, \n",
    "       n_jobs=10,\n",
    "       use_gpu=True,\n",
    "       preprocess=False,\n",
    "       normalize=False,\n",
    "       verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Initiated</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>09:08:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Status</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Loading Dependencies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estimator</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Compiling Library</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    \n",
       "                                                                    \n",
       "Initiated  . . . . . . . . . . . . . . . . . .              09:08:36\n",
       "Status     . . . . . . . . . . . . . . . . . .  Loading Dependencies\n",
       "Estimator  . . . . . . . . . . . . . . . . . .     Compiling Library"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d97a4_row10_col0, #T_d97a4_row10_col1, #T_d97a4_row10_col2, #T_d97a4_row10_col3, #T_d97a4_row10_col4, #T_d97a4_row10_col5, #T_d97a4_row10_col6 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d97a4\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d97a4_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_d97a4_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_d97a4_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_d97a4_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_d97a4_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_d97a4_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_d97a4_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d97a4_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d97a4_row0_col0\" class=\"data row0 col0\" >0.8024</td>\n",
       "      <td id=\"T_d97a4_row0_col1\" class=\"data row0 col1\" >0.8129</td>\n",
       "      <td id=\"T_d97a4_row0_col2\" class=\"data row0 col2\" >0.8024</td>\n",
       "      <td id=\"T_d97a4_row0_col3\" class=\"data row0 col3\" >0.7775</td>\n",
       "      <td id=\"T_d97a4_row0_col4\" class=\"data row0 col4\" >0.7545</td>\n",
       "      <td id=\"T_d97a4_row0_col5\" class=\"data row0 col5\" >0.2448</td>\n",
       "      <td id=\"T_d97a4_row0_col6\" class=\"data row0 col6\" >0.3078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d97a4_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_d97a4_row1_col0\" class=\"data row1 col0\" >0.8034</td>\n",
       "      <td id=\"T_d97a4_row1_col1\" class=\"data row1 col1\" >0.8095</td>\n",
       "      <td id=\"T_d97a4_row1_col2\" class=\"data row1 col2\" >0.8034</td>\n",
       "      <td id=\"T_d97a4_row1_col3\" class=\"data row1 col3\" >0.7786</td>\n",
       "      <td id=\"T_d97a4_row1_col4\" class=\"data row1 col4\" >0.7565</td>\n",
       "      <td id=\"T_d97a4_row1_col5\" class=\"data row1 col5\" >0.2512</td>\n",
       "      <td id=\"T_d97a4_row1_col6\" class=\"data row1 col6\" >0.3134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d97a4_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_d97a4_row2_col0\" class=\"data row2 col0\" >0.8056</td>\n",
       "      <td id=\"T_d97a4_row2_col1\" class=\"data row2 col1\" >0.8122</td>\n",
       "      <td id=\"T_d97a4_row2_col2\" class=\"data row2 col2\" >0.8056</td>\n",
       "      <td id=\"T_d97a4_row2_col3\" class=\"data row2 col3\" >0.7827</td>\n",
       "      <td id=\"T_d97a4_row2_col4\" class=\"data row2 col4\" >0.7587</td>\n",
       "      <td id=\"T_d97a4_row2_col5\" class=\"data row2 col5\" >0.2586</td>\n",
       "      <td id=\"T_d97a4_row2_col6\" class=\"data row2 col6\" >0.3233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d97a4_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_d97a4_row3_col0\" class=\"data row3 col0\" >0.8032</td>\n",
       "      <td id=\"T_d97a4_row3_col1\" class=\"data row3 col1\" >0.8116</td>\n",
       "      <td id=\"T_d97a4_row3_col2\" class=\"data row3 col2\" >0.8032</td>\n",
       "      <td id=\"T_d97a4_row3_col3\" class=\"data row3 col3\" >0.7798</td>\n",
       "      <td id=\"T_d97a4_row3_col4\" class=\"data row3 col4\" >0.7546</td>\n",
       "      <td id=\"T_d97a4_row3_col5\" class=\"data row3 col5\" >0.2445</td>\n",
       "      <td id=\"T_d97a4_row3_col6\" class=\"data row3 col6\" >0.3096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d97a4_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_d97a4_row4_col0\" class=\"data row4 col0\" >0.8038</td>\n",
       "      <td id=\"T_d97a4_row4_col1\" class=\"data row4 col1\" >0.8132</td>\n",
       "      <td id=\"T_d97a4_row4_col2\" class=\"data row4 col2\" >0.8038</td>\n",
       "      <td id=\"T_d97a4_row4_col3\" class=\"data row4 col3\" >0.7790</td>\n",
       "      <td id=\"T_d97a4_row4_col4\" class=\"data row4 col4\" >0.7569</td>\n",
       "      <td id=\"T_d97a4_row4_col5\" class=\"data row4 col5\" >0.2531</td>\n",
       "      <td id=\"T_d97a4_row4_col6\" class=\"data row4 col6\" >0.3157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d97a4_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_d97a4_row5_col0\" class=\"data row5 col0\" >0.8052</td>\n",
       "      <td id=\"T_d97a4_row5_col1\" class=\"data row5 col1\" >0.8136</td>\n",
       "      <td id=\"T_d97a4_row5_col2\" class=\"data row5 col2\" >0.8052</td>\n",
       "      <td id=\"T_d97a4_row5_col3\" class=\"data row5 col3\" >0.7833</td>\n",
       "      <td id=\"T_d97a4_row5_col4\" class=\"data row5 col4\" >0.7578</td>\n",
       "      <td id=\"T_d97a4_row5_col5\" class=\"data row5 col5\" >0.2557</td>\n",
       "      <td id=\"T_d97a4_row5_col6\" class=\"data row5 col6\" >0.3214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d97a4_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_d97a4_row6_col0\" class=\"data row6 col0\" >0.8051</td>\n",
       "      <td id=\"T_d97a4_row6_col1\" class=\"data row6 col1\" >0.8110</td>\n",
       "      <td id=\"T_d97a4_row6_col2\" class=\"data row6 col2\" >0.8051</td>\n",
       "      <td id=\"T_d97a4_row6_col3\" class=\"data row6 col3\" >0.7830</td>\n",
       "      <td id=\"T_d97a4_row6_col4\" class=\"data row6 col4\" >0.7571</td>\n",
       "      <td id=\"T_d97a4_row6_col5\" class=\"data row6 col5\" >0.2529</td>\n",
       "      <td id=\"T_d97a4_row6_col6\" class=\"data row6 col6\" >0.3198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d97a4_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_d97a4_row7_col0\" class=\"data row7 col0\" >0.8035</td>\n",
       "      <td id=\"T_d97a4_row7_col1\" class=\"data row7 col1\" >0.8142</td>\n",
       "      <td id=\"T_d97a4_row7_col2\" class=\"data row7 col2\" >0.8035</td>\n",
       "      <td id=\"T_d97a4_row7_col3\" class=\"data row7 col3\" >0.7807</td>\n",
       "      <td id=\"T_d97a4_row7_col4\" class=\"data row7 col4\" >0.7544</td>\n",
       "      <td id=\"T_d97a4_row7_col5\" class=\"data row7 col5\" >0.2429</td>\n",
       "      <td id=\"T_d97a4_row7_col6\" class=\"data row7 col6\" >0.3106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d97a4_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_d97a4_row8_col0\" class=\"data row8 col0\" >0.8046</td>\n",
       "      <td id=\"T_d97a4_row8_col1\" class=\"data row8 col1\" >0.8088</td>\n",
       "      <td id=\"T_d97a4_row8_col2\" class=\"data row8 col2\" >0.8046</td>\n",
       "      <td id=\"T_d97a4_row8_col3\" class=\"data row8 col3\" >0.7814</td>\n",
       "      <td id=\"T_d97a4_row8_col4\" class=\"data row8 col4\" >0.7585</td>\n",
       "      <td id=\"T_d97a4_row8_col5\" class=\"data row8 col5\" >0.2580</td>\n",
       "      <td id=\"T_d97a4_row8_col6\" class=\"data row8 col6\" >0.3201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d97a4_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_d97a4_row9_col0\" class=\"data row9 col0\" >0.8047</td>\n",
       "      <td id=\"T_d97a4_row9_col1\" class=\"data row9 col1\" >0.8127</td>\n",
       "      <td id=\"T_d97a4_row9_col2\" class=\"data row9 col2\" >0.8047</td>\n",
       "      <td id=\"T_d97a4_row9_col3\" class=\"data row9 col3\" >0.7819</td>\n",
       "      <td id=\"T_d97a4_row9_col4\" class=\"data row9 col4\" >0.7584</td>\n",
       "      <td id=\"T_d97a4_row9_col5\" class=\"data row9 col5\" >0.2571</td>\n",
       "      <td id=\"T_d97a4_row9_col6\" class=\"data row9 col6\" >0.3199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d97a4_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_d97a4_row10_col0\" class=\"data row10 col0\" >0.8041</td>\n",
       "      <td id=\"T_d97a4_row10_col1\" class=\"data row10 col1\" >0.8120</td>\n",
       "      <td id=\"T_d97a4_row10_col2\" class=\"data row10 col2\" >0.8041</td>\n",
       "      <td id=\"T_d97a4_row10_col3\" class=\"data row10 col3\" >0.7808</td>\n",
       "      <td id=\"T_d97a4_row10_col4\" class=\"data row10 col4\" >0.7567</td>\n",
       "      <td id=\"T_d97a4_row10_col5\" class=\"data row10 col5\" >0.2519</td>\n",
       "      <td id=\"T_d97a4_row10_col6\" class=\"data row10 col6\" >0.3162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d97a4_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_d97a4_row11_col0\" class=\"data row11 col0\" >0.0010</td>\n",
       "      <td id=\"T_d97a4_row11_col1\" class=\"data row11 col1\" >0.0017</td>\n",
       "      <td id=\"T_d97a4_row11_col2\" class=\"data row11 col2\" >0.0010</td>\n",
       "      <td id=\"T_d97a4_row11_col3\" class=\"data row11 col3\" >0.0019</td>\n",
       "      <td id=\"T_d97a4_row11_col4\" class=\"data row11 col4\" >0.0016</td>\n",
       "      <td id=\"T_d97a4_row11_col5\" class=\"data row11 col5\" >0.0056</td>\n",
       "      <td id=\"T_d97a4_row11_col6\" class=\"data row11 col6\" >0.0052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b1859ecb80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d97a4_row10_col0, #T_d97a4_row10_col1, #T_d97a4_row10_col2, #T_d97a4_row10_col3, #T_d97a4_row10_col4, #T_d97a4_row10_col5, #T_d97a4_row10_col6 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d97a4\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d97a4_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_d97a4_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_d97a4_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_d97a4_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_d97a4_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_d97a4_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_d97a4_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d97a4_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d97a4_row0_col0\" class=\"data row0 col0\" >0.8024</td>\n",
       "      <td id=\"T_d97a4_row0_col1\" class=\"data row0 col1\" >0.8129</td>\n",
       "      <td id=\"T_d97a4_row0_col2\" class=\"data row0 col2\" >0.8024</td>\n",
       "      <td id=\"T_d97a4_row0_col3\" class=\"data row0 col3\" >0.7775</td>\n",
       "      <td id=\"T_d97a4_row0_col4\" class=\"data row0 col4\" >0.7545</td>\n",
       "      <td id=\"T_d97a4_row0_col5\" class=\"data row0 col5\" >0.2448</td>\n",
       "      <td id=\"T_d97a4_row0_col6\" class=\"data row0 col6\" >0.3078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d97a4_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_d97a4_row1_col0\" class=\"data row1 col0\" >0.8034</td>\n",
       "      <td id=\"T_d97a4_row1_col1\" class=\"data row1 col1\" >0.8095</td>\n",
       "      <td id=\"T_d97a4_row1_col2\" class=\"data row1 col2\" >0.8034</td>\n",
       "      <td id=\"T_d97a4_row1_col3\" class=\"data row1 col3\" >0.7786</td>\n",
       "      <td id=\"T_d97a4_row1_col4\" class=\"data row1 col4\" >0.7565</td>\n",
       "      <td id=\"T_d97a4_row1_col5\" class=\"data row1 col5\" >0.2512</td>\n",
       "      <td id=\"T_d97a4_row1_col6\" class=\"data row1 col6\" >0.3134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d97a4_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_d97a4_row2_col0\" class=\"data row2 col0\" >0.8056</td>\n",
       "      <td id=\"T_d97a4_row2_col1\" class=\"data row2 col1\" >0.8122</td>\n",
       "      <td id=\"T_d97a4_row2_col2\" class=\"data row2 col2\" >0.8056</td>\n",
       "      <td id=\"T_d97a4_row2_col3\" class=\"data row2 col3\" >0.7827</td>\n",
       "      <td id=\"T_d97a4_row2_col4\" class=\"data row2 col4\" >0.7587</td>\n",
       "      <td id=\"T_d97a4_row2_col5\" class=\"data row2 col5\" >0.2586</td>\n",
       "      <td id=\"T_d97a4_row2_col6\" class=\"data row2 col6\" >0.3233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d97a4_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_d97a4_row3_col0\" class=\"data row3 col0\" >0.8032</td>\n",
       "      <td id=\"T_d97a4_row3_col1\" class=\"data row3 col1\" >0.8116</td>\n",
       "      <td id=\"T_d97a4_row3_col2\" class=\"data row3 col2\" >0.8032</td>\n",
       "      <td id=\"T_d97a4_row3_col3\" class=\"data row3 col3\" >0.7798</td>\n",
       "      <td id=\"T_d97a4_row3_col4\" class=\"data row3 col4\" >0.7546</td>\n",
       "      <td id=\"T_d97a4_row3_col5\" class=\"data row3 col5\" >0.2445</td>\n",
       "      <td id=\"T_d97a4_row3_col6\" class=\"data row3 col6\" >0.3096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d97a4_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_d97a4_row4_col0\" class=\"data row4 col0\" >0.8038</td>\n",
       "      <td id=\"T_d97a4_row4_col1\" class=\"data row4 col1\" >0.8132</td>\n",
       "      <td id=\"T_d97a4_row4_col2\" class=\"data row4 col2\" >0.8038</td>\n",
       "      <td id=\"T_d97a4_row4_col3\" class=\"data row4 col3\" >0.7790</td>\n",
       "      <td id=\"T_d97a4_row4_col4\" class=\"data row4 col4\" >0.7569</td>\n",
       "      <td id=\"T_d97a4_row4_col5\" class=\"data row4 col5\" >0.2531</td>\n",
       "      <td id=\"T_d97a4_row4_col6\" class=\"data row4 col6\" >0.3157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d97a4_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_d97a4_row5_col0\" class=\"data row5 col0\" >0.8052</td>\n",
       "      <td id=\"T_d97a4_row5_col1\" class=\"data row5 col1\" >0.8136</td>\n",
       "      <td id=\"T_d97a4_row5_col2\" class=\"data row5 col2\" >0.8052</td>\n",
       "      <td id=\"T_d97a4_row5_col3\" class=\"data row5 col3\" >0.7833</td>\n",
       "      <td id=\"T_d97a4_row5_col4\" class=\"data row5 col4\" >0.7578</td>\n",
       "      <td id=\"T_d97a4_row5_col5\" class=\"data row5 col5\" >0.2557</td>\n",
       "      <td id=\"T_d97a4_row5_col6\" class=\"data row5 col6\" >0.3214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d97a4_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_d97a4_row6_col0\" class=\"data row6 col0\" >0.8051</td>\n",
       "      <td id=\"T_d97a4_row6_col1\" class=\"data row6 col1\" >0.8110</td>\n",
       "      <td id=\"T_d97a4_row6_col2\" class=\"data row6 col2\" >0.8051</td>\n",
       "      <td id=\"T_d97a4_row6_col3\" class=\"data row6 col3\" >0.7830</td>\n",
       "      <td id=\"T_d97a4_row6_col4\" class=\"data row6 col4\" >0.7571</td>\n",
       "      <td id=\"T_d97a4_row6_col5\" class=\"data row6 col5\" >0.2529</td>\n",
       "      <td id=\"T_d97a4_row6_col6\" class=\"data row6 col6\" >0.3198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d97a4_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_d97a4_row7_col0\" class=\"data row7 col0\" >0.8035</td>\n",
       "      <td id=\"T_d97a4_row7_col1\" class=\"data row7 col1\" >0.8142</td>\n",
       "      <td id=\"T_d97a4_row7_col2\" class=\"data row7 col2\" >0.8035</td>\n",
       "      <td id=\"T_d97a4_row7_col3\" class=\"data row7 col3\" >0.7807</td>\n",
       "      <td id=\"T_d97a4_row7_col4\" class=\"data row7 col4\" >0.7544</td>\n",
       "      <td id=\"T_d97a4_row7_col5\" class=\"data row7 col5\" >0.2429</td>\n",
       "      <td id=\"T_d97a4_row7_col6\" class=\"data row7 col6\" >0.3106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d97a4_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_d97a4_row8_col0\" class=\"data row8 col0\" >0.8046</td>\n",
       "      <td id=\"T_d97a4_row8_col1\" class=\"data row8 col1\" >0.8088</td>\n",
       "      <td id=\"T_d97a4_row8_col2\" class=\"data row8 col2\" >0.8046</td>\n",
       "      <td id=\"T_d97a4_row8_col3\" class=\"data row8 col3\" >0.7814</td>\n",
       "      <td id=\"T_d97a4_row8_col4\" class=\"data row8 col4\" >0.7585</td>\n",
       "      <td id=\"T_d97a4_row8_col5\" class=\"data row8 col5\" >0.2580</td>\n",
       "      <td id=\"T_d97a4_row8_col6\" class=\"data row8 col6\" >0.3201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d97a4_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_d97a4_row9_col0\" class=\"data row9 col0\" >0.8047</td>\n",
       "      <td id=\"T_d97a4_row9_col1\" class=\"data row9 col1\" >0.8127</td>\n",
       "      <td id=\"T_d97a4_row9_col2\" class=\"data row9 col2\" >0.8047</td>\n",
       "      <td id=\"T_d97a4_row9_col3\" class=\"data row9 col3\" >0.7819</td>\n",
       "      <td id=\"T_d97a4_row9_col4\" class=\"data row9 col4\" >0.7584</td>\n",
       "      <td id=\"T_d97a4_row9_col5\" class=\"data row9 col5\" >0.2571</td>\n",
       "      <td id=\"T_d97a4_row9_col6\" class=\"data row9 col6\" >0.3199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d97a4_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_d97a4_row10_col0\" class=\"data row10 col0\" >0.8041</td>\n",
       "      <td id=\"T_d97a4_row10_col1\" class=\"data row10 col1\" >0.8120</td>\n",
       "      <td id=\"T_d97a4_row10_col2\" class=\"data row10 col2\" >0.8041</td>\n",
       "      <td id=\"T_d97a4_row10_col3\" class=\"data row10 col3\" >0.7808</td>\n",
       "      <td id=\"T_d97a4_row10_col4\" class=\"data row10 col4\" >0.7567</td>\n",
       "      <td id=\"T_d97a4_row10_col5\" class=\"data row10 col5\" >0.2519</td>\n",
       "      <td id=\"T_d97a4_row10_col6\" class=\"data row10 col6\" >0.3162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d97a4_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_d97a4_row11_col0\" class=\"data row11 col0\" >0.0010</td>\n",
       "      <td id=\"T_d97a4_row11_col1\" class=\"data row11 col1\" >0.0017</td>\n",
       "      <td id=\"T_d97a4_row11_col2\" class=\"data row11 col2\" >0.0010</td>\n",
       "      <td id=\"T_d97a4_row11_col3\" class=\"data row11 col3\" >0.0019</td>\n",
       "      <td id=\"T_d97a4_row11_col4\" class=\"data row11 col4\" >0.0016</td>\n",
       "      <td id=\"T_d97a4_row11_col5\" class=\"data row11 col5\" >0.0056</td>\n",
       "      <td id=\"T_d97a4_row11_col6\" class=\"data row11 col6\" >0.0052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b1859ecb80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lightgbm= create_model('lightgbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_model(lightgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "965e648887e34a3ca9ed53ed530b48ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(ToggleButtons(description='Plot Type:', icons=('',), options=(('Pipeline Plot', 'pipelinâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_model(lightgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Pipeline(memory=FastMemory(location=C:\\Users\\31405~1.ISB\\AppData\\Local\\Temp\\12\\joblib),\n",
       "          steps=[('placeholder', None),\n",
       "                 ('trained_model',\n",
       "                  LGBMClassifier(boosting_type='gbdt', class_weight=None,\n",
       "                                 colsample_bytree=1.0, device='gpu',\n",
       "                                 importance_type='split', learning_rate=0.1,\n",
       "                                 max_depth=-1, min_child_samples=20,\n",
       "                                 min_child_weight=0.001, min_split_gain=0.0,\n",
       "                                 n_estimators=100, n_jobs=45, num_leaves=31,\n",
       "                                 objective=None, random_state=125, reg_alpha=0.0,\n",
       "                                 reg_lambda=0.0, silent='warn', subsample=1.0,\n",
       "                                 subsample_for_bin=200000, subsample_freq=0))],\n",
       "          verbose=False),\n",
       " 'LightGBM.pkl')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(best, 'LightGBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_app(lightgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'submitted_via_le': {0: 5},\n",
       " 'timely_response_le': {0: 1},\n",
       " 'year': {0: 2015},\n",
       " 'month': {0: 10},\n",
       " 'day': {0: 0},\n",
       " 'product_le': {0: 4},\n",
       " 'sub_product_le': {0: 20.0},\n",
       " 'issue_le': {0: 25},\n",
       " 'sub_issue_le': {0: 20.0},\n",
       " 'company_ce': {0: 0.0010132158590308},\n",
       " 'state_le': {0: 23},\n",
       " 'consumer_consent_provided_le': {0: 1.0},\n",
       " 'company_response_to_consumer_le': {0: 0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(1).to_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
